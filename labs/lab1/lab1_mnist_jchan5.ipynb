{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11767 ODML Lab 1 - jchan5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.2 Define constants and experimental setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n",
      "Python version: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]\n",
      "Pytorch version: 2.2.0+cpu\n",
      "Processor 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz   2.30 GHz\n",
      "Installed RAM\t32.0 GB (31.8 GB usable\n",
      "Windows 11 Home 64-bit operating system, x64-based processor\n"
     ]
    }
   ],
   "source": [
    "img_size = (28, 28)\n",
    "num_labels = 10\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "num_layers = 2\n",
    "hidden_size = 1024\n",
    "num_epochs = 2\n",
    "# print(type(img_size[0] * img_size[1]))\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using device {device}\")\n",
    "# print the python version \n",
    "print(f\"Python version: {sys.version}\")\n",
    "# and pytorch version\n",
    "print(f\"Pytorch version: {torch.__version__}\")\n",
    "# operating system\n",
    "print(\"Processor 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz   2.30 GHz\")\n",
    "print(\"Installed RAM\t32.0 GB (31.8 GB usable\")\n",
    "print(\"Windows 11 Home 64-bit operating system, x64-based processor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CsvMNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data_frame.iloc[idx]\n",
    "        label = row[0] # first value is the class label\n",
    "        img = row[1:].values.astype(\"uint8\").reshape(img_size) # reshape 28x28\n",
    "        img = Image.fromarray(img, mode=\"L\") # L = 8bit greyscale\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define dataloader and preprocess inputs to have µ=0 and σ=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def get_data(batch_size):\n",
    "    transform_mnist = T.Compose([\n",
    "        T.ToTensor(), \n",
    "        T.Resize(min(img_size[0], img_size[1]), antialias=True),  # Resize the smallest side to 256 pixels\n",
    "        T.CenterCrop(img_size),  # Center crop to 256x256\n",
    "        T.Normalize(mean=[0], std=[1]) # Normalize to 0 mean and 1 std\n",
    "        ])\n",
    "    train_data = CsvMNISTDataset(\n",
    "        csv_file='./mnist_data/mnist_train.csv',\n",
    "        transform=transform_mnist,\n",
    "    )\n",
    "    test_data = CsvMNISTDataset(\n",
    "        csv_file='./mnist_data/mnist_test.csv',\n",
    "        transform=transform_mnist,\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "    \n",
    "    for X, y in train_dataloader:\n",
    "        print(f\"Shape of X [B, C, H, W]: {X.shape}\") # [batch_size, channels, dims]\n",
    "        print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "        break\n",
    "    \n",
    "    return train_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model architecture according to\n",
    "\n",
    "| Hyperparameter    | Value |\n",
    "| --------- | --------- |\n",
    "| Learning Rate\t| 0.001 |\n",
    "| Batch Size | 64 |\n",
    "| Hidden Layers |\t2 |\n",
    "| Hidden Size\t| 1024 |\n",
    "| Epochs | 2 |\n",
    "\n",
    "Q2.3 Count the number of FLOPs in countflops()\n",
    "\n",
    "The NN is composed an input layer, two hidden layers, and an output layer. Since addition and multiplication count as one operation, the number of FLOPs in a FF single layer is the product of the input dimension squared times the output dimension. For example, the first layer takes an input of 784 and multiplies that with the first layer's 784 weights, doing this 1024 times. The output is of size 1024, where a bias term is added to each element. \n",
    "\n",
    "After those 1024 additions, the ReLU layer performs a comparison against 0 for each input to the layer and stores an output value depending on the result for a total of 2 x 1024 operations\n",
    "\n",
    "Symbolically, a FF network of a layer size [A, B] will have A x A x B + B FLOPs and 2B FLOPs for the ReLU layer, which is represented below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTNetwork(nn.Module):\n",
    "    def __init__(self, hidden_layers=2, hidden_size=1024, num_labels=10):\n",
    "        super(MNISTNetwork, self).__init__()\n",
    "        # First layer input size must be the dimension of the image\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Define NN layers based on the number of layers and hidden size\n",
    "        flatten_size = img_size[0] * img_size[1] # int\n",
    "        self.NN_layers = []\n",
    "        self.NN_layers.append(flatten_size) # first element is input\n",
    "        for i in range(hidden_layers):\n",
    "            self.NN_layers.append(hidden_size)\n",
    "        self.NN_layers.append(num_labels) # output layer size\n",
    "        NN = []\n",
    "        for i in range(len(self.NN_layers)-1):\n",
    "            # [784, 1024] -> ReLU -> [1024, 1024] -> ReLU -> [1024, 10]\n",
    "            NN.append(nn.Linear(self.NN_layers[i],self.NN_layers[i+1]))\n",
    "            if i < (hidden_layers):\n",
    "                NN.append(nn.ReLU())\n",
    "        self.sequential = nn.Sequential(*NN)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.sequential(x)\n",
    "        return logits\n",
    "    \n",
    "    def countflops(self):\n",
    "        # Count FLOPs per layer [784,1024,1024,10]\n",
    "        # Linear layer: Ax + b\n",
    "        # ReLU: b\n",
    "        # dim(A)[0] * dim(A)[1] + dim(B) + dim(B)\n",
    "        flop_count = 0\n",
    "        for i in range(len(self.NN_layers)-1): # 0 to 2\n",
    "            A = self.NN_layers[i]\n",
    "            B = self.NN_layers[i+1]\n",
    "            flop_count += A*A*B+B # linear layer\n",
    "            if i < (len(self.NN_layers)-2): # before output layer no ReLU\n",
    "                flop_count += 2*B\n",
    "        print(f\"FLOPs: {flop_count}\")\n",
    "        return flop_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.1 Define training routine and measure training and inference latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloader, model, loss_fn, optimizer, cur_epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    batch_size = dataloader.batch_size\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss.item() / batch_size\n",
    "        current = (batch + 1) * dataloader.batch_size # number of examples seen in this epoch\n",
    "        if batch % 500 == 0:\n",
    "            print(f\"Train loss = {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    end_time = time.time()\n",
    "    train_epoch_duration = end_time - start_time\n",
    "    # print(f\"Epoch {cur_epoch+1} training duration: {train_epoch_duration}\")\n",
    "    return train_epoch_duration\n",
    "\n",
    "# Evaluate train accuracy and loss\n",
    "def evaluate(dataloader, dataname, model, loss_fn, cur_epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    #start_time = time.time()\n",
    "    model.eval()\n",
    "    avg_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            avg_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    #end_time = time.time()\n",
    "    avg_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"{dataname} accuracy = {(100*correct):>0.1f}%, {dataname} avg loss = {avg_loss:>8f}\")\n",
    "\n",
    "def test_evaluate(dataloader, dataname, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    avg_loss, correct, inference_latency, count = 0, 0, 0, 0\n",
    "    # discard first few iterations\n",
    "    warmup = 3\n",
    "    infer_times = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            count += 1\n",
    "            num_samples = len(y)\n",
    "            if count > warmup:\n",
    "                start_time = time.time()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            avg_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            if count > warmup:\n",
    "                end_time = time.time()\n",
    "                inference_latency += (end_time - start_time)\n",
    "                infer_times.append((end_time-start_time)/num_samples)# 1 sample\n",
    "    avg_loss /= size\n",
    "    correct /= size\n",
    "    avg_time_per_inference = inference_latency / (size - num_samples*warmup)\n",
    "    print(f\"{dataname} accuracy = {(100*correct):>0.1f}%, {dataname} avg loss = {avg_loss:>8f}\")\n",
    "    print(f\"Average time per example classification: {avg_time_per_inference:>5f} seconds\")\n",
    "    return infer_times, avg_time_per_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Shape of X [B, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n",
      "MNISTNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (sequential): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joncc\\AppData\\Local\\Temp\\ipykernel_10300\\2121649383.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  label = row[0] # first value is the class label\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using {device} device\")\n",
    "train_dataloader, test_dataloader = get_data(batch_size)\n",
    "train_size = len(train_dataloader.dataset)\n",
    "test_size = len(test_dataloader.dataset)\n",
    "model = MNISTNetwork().to(device)\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss() # no need to softmax as CrossEntropyLoss works on raw logits\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.1 Print training progress and accuracy\n",
    "\n",
    "The training accuracy is shown below with the run's corresponding hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joncc\\AppData\\Local\\Temp\\ipykernel_10300\\2121649383.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  label = row[0] # first value is the class label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Train loss = 0.036006  [   64/59999]\n",
      "Train loss = 0.004810  [32064/59999]\n",
      "Train accuracy = 95.5%, Train avg loss = 0.002348\n",
      "Test accuracy = 94.8%, Test avg loss = 0.002592\n",
      "Average time per example classification: 0.000028 seconds\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train loss = 0.001919  [   64/59999]\n",
      "Train loss = 0.002819  [32064/59999]\n",
      "Train accuracy = 97.5%, Train avg loss = 0.001237\n",
      "Test accuracy = 96.5%, Test avg loss = 0.001795\n",
      "Average time per example classification: 0.000026 seconds\n"
     ]
    }
   ],
   "source": [
    "# Main training \n",
    "epoch_train_times = []\n",
    "epoch_test_times = []\n",
    "average_inference_time = []\n",
    "for t in range(num_epochs):\n",
    "    print(f\"\\nEpoch {t+1}\\n-------------------------------\")\n",
    "    train_duration = train_one_epoch(train_dataloader, model, loss_fn, optimizer, t)\n",
    "    epoch_train_times.append(train_duration)\n",
    "    evaluate(train_dataloader, \"Train\", model, loss_fn, t)\n",
    "    epoch_test, avg = test_evaluate(test_dataloader, \"Test\", model, loss_fn)\n",
    "    epoch_test_times.append(epoch_test)\n",
    "    average_inference_time.append(avg)\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"MNIST_model.pth\")\n",
    "# print(\"Saved PyTorch Model State to MNIST_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: learning_rate=0.001, \n",
      " batch_size=64, num_layers=2, hidden_size=1024, num_epochs=2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hyperparameters: learning_rate={learning_rate}, \\n batch_size={batch_size}, num_layers={num_layers}, hidden_size={hidden_size}, num_epochs={num_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.3 Print the training time per epoch and inference latency per example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training duration: 37.043280\n",
      "Epoch 2 training duration: 41.538258\n",
      "Training time per epoch variance 5.051205\n"
     ]
    }
   ],
   "source": [
    "for t in range(num_epochs):\n",
    "    print(f\"Epoch {t+1} training duration: {epoch_train_times[t]:>8f}\")\n",
    "print(f\"Training time per epoch variance {np.array(epoch_train_times).var():>8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With two epochs, the training time per epoch has a variance of ~1 seconds. Training took almost 5% longer for the second epoch, which is surprising given that a warmed up CPU should compute quicker as the intuition behind inference goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average inference time per sample: 0.000028\n",
      "Epoch 2 average inference time per sample: 0.000026\n",
      "Test time per sample variance for epoch 1: 4.173304938698072e-10 seconds\n",
      "Test time per sample variance for epoch 2: 1.5022253472545984e-10 seconds\n",
      "Test time per sample max for epoch 1: 0.000240 seconds\n",
      "Test time per sample max for epoch 2: 0.000141 seconds\n",
      "Test time per sample min for epoch 1: 1.547485589981079e-05 seconds\n",
      "Test time per sample min for epoch 2: 1.5385448932647705e-05 seconds\n"
     ]
    }
   ],
   "source": [
    "for t in range(num_epochs):\n",
    "    print(f\"Epoch {t+1} average inference time per sample: {average_inference_time[t]:>8f}\")\n",
    "\n",
    "test_var1 = np.array(epoch_test_times[0]).var()\n",
    "test_var2 = np.array(epoch_test_times[1]).var()\n",
    "test_max1 = np.array(epoch_test_times[0]).max()\n",
    "test_max2 = np.array(epoch_test_times[1]).max()\n",
    "test_min1 = np.array(epoch_test_times[0]).min()\n",
    "test_min2 = np.array(epoch_test_times[1]).min()\n",
    "print(f\"Test time per sample variance for epoch 1: {test_var1} seconds\")\n",
    "print(f\"Test time per sample variance for epoch 2: {test_var2} seconds\")\n",
    "print(f\"Test time per sample max for epoch 1: {test_max1:>8f} seconds\")\n",
    "print(f\"Test time per sample max for epoch 2: {test_max2:>8f} seconds\")\n",
    "print(f\"Test time per sample min for epoch 1: {test_min1} seconds\")\n",
    "print(f\"Test time per sample min for epoch 2: {test_min2} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x196d4e23d90>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "xvals = np.array([1, 2, 3, 4, 5])\n",
    "yvals = np.array([1, 4, 9, 16, 25])\n",
    "plt.plot(xvals, yvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance for the inferences across the two epochs by default are as shown. What was suprising is that the second epoch has a larger max value for the per sample inference time. The first iteration of inference was not slower than the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.4 Count number of trainable parameters in a function\n",
    "\n",
    "The values line up as manually calculated as the weight matrices and the bias are the only trainable parameters in a feedforward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "\ttotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\tprint(f\"Total number of model parameters: {total_params}\")\n",
    "\n",
    "count_parameters(model)\n",
    "\n",
    "# manually computing parameters\n",
    "# 784*1024 + 1024 + 1024*1024 + 1024 + 1024*10 + 10 = 1863690\n",
    "NN_layers = [784, 1024, 1024, 10]\n",
    "total_params = 0\n",
    "for i in range(len(NN_layers)-1):\n",
    "\ttotal_params += NN_layers[i] * NN_layers[i+1] + NN_layers[i+1]\n",
    "print(f'Manually computed total number of model parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.5 Count flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_flops = model.countflops()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.6 Tune model hyperparameters\n",
    "\n",
    "The three plots to produce are generated with respect to training 4 other various architectures for two epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 6) (4147344663.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(f\"\\n\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 6)\n"
     ]
    }
   ],
   "source": [
    "hid_layers = [1, 2, 3]\n",
    "hid_size = [512, 1024]\n",
    "\n",
    "# log the late\n",
    "for nL in hid_layers:\n",
    "    for nH in hid_size:\n",
    "        model = MNISTNetwork().to(device)\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss() # no need to softmax as CrossEntropyLoss works on raw logits\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py32",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
