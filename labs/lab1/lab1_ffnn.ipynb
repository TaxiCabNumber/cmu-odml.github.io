{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install/check dependencies\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data and define dataloader \n",
    "class CsvTextDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= self.__len__(): raise IndexError()\n",
    "        text = self.data_frame.loc[idx, \"article\"]\n",
    "        label = self.data_frame.loc[idx, \"label_idx\"]\n",
    "\n",
    "        if self.transform:\n",
    "            text = self.transform(text)\n",
    "\n",
    "        return text, label\n",
    "    \n",
    "\n",
    "class CorpusInfo():\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.oov_token = '<UNK>'\n",
    "        self.pad_token = '<PAD>'\n",
    "        \n",
    "        def yield_tokens(data_iter):\n",
    "            for text, _ in data_iter:\n",
    "                yield tokenizer(text)\n",
    "        self.vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[self.oov_token, self.pad_token])\n",
    "        self.vocab.set_default_index(self.vocab[self.oov_token])\n",
    "        \n",
    "        self.oov_idx = self.vocab[self.oov_token]\n",
    "        self.pad_idx = self.vocab[self.pad_token]\n",
    "        \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.num_labels = len(set([label for (text, label) in dataset]))\n",
    "\n",
    "class TextTransform(torch.Callable):\n",
    "    def __init__(self, tokenizer, vocab):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def tokenize_and_numericalize(self, text):\n",
    "        tokens = self.tokenizer(text)\n",
    "        return [self.vocab[token] for token in tokens]\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.tokenize_and_numericalize(text)\n",
    "    \n",
    "class MaxLen(torch.Callable):\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        if len(x) > self.max_len:\n",
    "            x = x[:self.max_len]\n",
    "        return x\n",
    "    \n",
    "class PadSequence(torch.Callable):\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        def to_int_tensor(x):\n",
    "            return torch.from_numpy(np.array(x, dtype=np.int64, copy=False))\n",
    "        # Convert each sequence of tokens to a Tensor\n",
    "        sequences = [to_int_tensor(x[0]) for x in batch]\n",
    "        # Convert the full sequence of labels to a Tensor\n",
    "        labels = to_int_tensor([x[1] for x in batch])\n",
    "        sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=self.pad_idx)\n",
    "        return sequences_padded, labels\n",
    "\n",
    "def get_data():    \n",
    "    train_data = CsvTextDataset(\n",
    "        csv_file='./data/txt_train.csv',\n",
    "        transform=None,\n",
    "    )\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    corpus_info = CorpusInfo(train_data, tokenizer)\n",
    "    transform_txt = T.Compose([\n",
    "        TextTransform(corpus_info.tokenizer, corpus_info.vocab),\n",
    "        MaxLen(MAX_LEN),\n",
    "    ])\n",
    "    train_data = CsvTextDataset(\n",
    "        csv_file='./data/txt_train.csv',\n",
    "        transform=transform_txt,\n",
    "    )\n",
    "    val_data = CsvTextDataset(\n",
    "        csv_file='./data/txt_val.csv',\n",
    "        transform=transform_txt,\n",
    "    )\n",
    "    test_data = CsvTextDataset(\n",
    "        csv_file='./data/txt_test.csv',\n",
    "        transform=transform_txt,\n",
    "    )\n",
    "\n",
    "    collate_batch = PadSequence(corpus_info.pad_idx)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
    "\n",
    "    for X, y in train_dataloader:\n",
    "        print(f\"Shape of X [B, N]: {X.shape}\")\n",
    "        print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "        break\n",
    "    \n",
    "    return corpus_info, train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
